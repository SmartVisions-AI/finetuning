# Experiment: SFT- Fine tuning 

## Description

Llama 8B Distill version of deepseek 

Models can also be found here: [Hugginface Collection](https://huggingface.co/collections/moslehGen/sft-681ccba9ae754cd58d2e9b2c)

## Contents

| Notebook | Description | Model Repo |
|----------|-------------|-------------|
| `Tourism_fine_tuning_deepseek_r1.ipynb` | 50K Dataset subset of the 166K recieved | moslehGen/DeepSeek-R1-Distill-Llama-8B-RunPOD-Rows-trained-50000
| `Tourism_fine_tuning_deepseek_r1 COT.ipynb` | 26K dataset, originally recieved with Chain-of-Though | moslehGen/SFT-COT-25K
| `Tourism_fine_tuning_deepseek_r1 100K.ipynb` | 100 K dataset with Balanced Sentiments | moslehGen/DeepSeek-R1-Distill-Llama-8B-RunPOD-Rows-trained-100000

## How to Run

Inference with Colab can be found here: [Colab](https://colab.research.google.com/drive/1y7ecU3swRg98_qW-EIL_AAq816Qrk5qD?usp=sharing)





